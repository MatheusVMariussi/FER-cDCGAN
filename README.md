# WGAN-GP Condicional "Heavy" para GeraÃ§Ã£o de ExpressÃµes Faciais

Este projeto implementa uma rede Generative Adversarial (GAN) condicional de alta performance, baseada em WGAN-GP, para gerar imagens de expressÃµes faciais em tons de cinza de 48x48 pixels.

O modelo utiliza uma arquitetura "pesada" (`Heavy`) que incorpora vÃ¡rias tÃ©cnicas modernas para estabilizar o treinamento e melhorar a qualidade da imagem, incluindo NormalizaÃ§Ã£o Espectral, Self-Attention e um design inspirado no StyleGAN.

## ğŸ–¼ï¸ Amostras Geradas

TODO: 
(Inserir aqui uma imagem de grade de amostras geradas, por exemplo, a `samples_epoch_500.png` que o script de treinamento salva)

`![Exemplos de imagens geradas](generated_heavy/samples_epoch_500.png)`

---

## ğŸš€ Arquitetura e Principais CaracterÃ­sticas

### 1. Gerador (`HeavyGenerator`)
* **Rede de Mapeamento (Estilo StyleGAN):** O vetor de ruÃ­do `z` e o *embedding* da classe nÃ£o sÃ£o alimentados diretamente na rede de convoluÃ§Ã£o. Eles sÃ£o primeiro processados por uma rede de mapeamento (MLP) para desacoplar o espaÃ§o latente.
* **Upsampling Progressivo:** O modelo comeÃ§a com uma constante 4x4 e usa `Upsample` (Nearest/Bilinear) seguido por convoluÃ§Ãµes para aumentar a resoluÃ§Ã£o (4x4 â†’ 8x8 â†’ 16x16 â†’ 32x32 â†’ 48x48).
* **NormalizaÃ§Ã£o por InstÃ¢ncia (`InstanceNorm2d`):** Usada em vez de `BatchNorm` para evitar a correlaÃ§Ã£o entre amostras do batch, o que Ã© comum em modelos de estilo.
* **Self-Attention:** Uma camada de `SelfAttention` Ã© aplicada na resoluÃ§Ã£o de 16x16 para permitir que o modelo aprenda dependÃªncias de longo alcance na imagem (ex: garantir que os dois olhos faÃ§am sentido juntos).

### 2. Discriminador (`HeavyDiscriminator`)
* **NormalizaÃ§Ã£o Espectral (`SpectralNorm`):** Aplicada a todas as camadas convolucionais para impor a restriÃ§Ã£o de Lipschitz (1-Lipschitz), que Ã© o nÃºcleo do WGAN-GP, garantindo um treinamento muito mais estÃ¡vel.
* **Downsampling Progressivo:** Reduz a imagem de 48x48 para 3x3 atravÃ©s de blocos `Conv2d` com `stride=2`.
* **Self-Attention:** TambÃ©m presente no discriminador (na resoluÃ§Ã£o 6x6) para ajudar a reforÃ§ar relaÃ§Ãµes estruturais complexas.
* **Minibatch Standard Deviation (`MinibatchStdDev`):** Uma tÃ©cnica poderosa para combater o colapso de modo. Ela adiciona um canal extra Ã s features, contendo a informaÃ§Ã£o da variabilidade do batch, permitindo ao discriminador penalizar o gerador se ele produzir amostras muito similares.
* **Condicionamento por ProjeÃ§Ã£o (Estilo cGAN-Proj):** O *embedding* da classe nÃ£o Ã© simplesmente concatenado Ã  imagem. Ele Ã© processado e concatenado a um estÃ¡gio intermediÃ¡rio (3x3) e usado em uma camada final de classificaÃ§Ã£o `Conv2d` para determinar a validade da imagem *dada* a classe.

### 3. EstratÃ©gia de Treinamento (`train_cdcgan.py`)
* **WGAN-GP:** Usa a perda Wasserstein com *Gradient Penalty* (GP) em vez da perda de entropia cruzada binÃ¡ria (BCELoss). Isso resulta em gradientes mais suaves e evita a saturaÃ§Ã£o do discriminador.
* **Treinamento AssimÃ©trico (`n_critic`):** O discriminador (crÃ­tico) Ã© treinado 5 vezes (`n_critic=5`) para cada atualizaÃ§Ã£o do gerador, conforme recomendado para WGANs.
* **Otimizador Adam:** Utiliza os betas `(0.0, 0.9)` recomendados para WGANs.
* **Exponential Moving Average (EMA):** O script mantÃ©m uma cÃ³pia de "sombra" (EMA) dos pesos do gerador. As amostras de imagem e os checkpoints finais sÃ£o salvos usando esta versÃ£o EMA, que geralmente produz resultados visualmente mais estÃ¡veis e de maior qualidade do que os pesos do gerador no Ãºltimo passo.

---

## ğŸ“ Estrutura do Projeto

```

.
â”œâ”€â”€ data/
â”‚   â””â”€â”€ fer2013/
â”‚       â””â”€â”€ train/
â”‚           â”œâ”€â”€ angry/
â”‚           â”œâ”€â”€ happy/
â”‚           â”œâ”€â”€ sad/
â”‚           â””â”€â”€ ... (outras emoÃ§Ãµes)
â”œâ”€â”€ checkpoints_heavy/   (SaÃ­da para checkpoints de modelo)
â”œâ”€â”€ generated_heavy/     (SaÃ­da para imagens de amostra e geraÃ§Ã£o)
â”œâ”€â”€ train_cdcgan.py      \# Script principal para treinar o modelo
â”œâ”€â”€ generate_samples.py  \# Script para gerar imagens com um modelo treinado
â”œâ”€â”€ models_heavy2.py     \# DefiniÃ§Ã£o das classes HeavyGenerator e HeavyDiscriminator
â”œâ”€â”€ datasets.py          \# Classes de Dataset (FERFolder, FERCsv)
â”œâ”€â”€ utils.py             \# FunÃ§Ãµes utilitÃ¡rias (salvar grade de imagens, etc.)
â””â”€â”€ requirements.txt     \# DependÃªncias do projeto

````

---

## ğŸ› ï¸ Como Usar


### 1\. Treinamento

Para iniciar o treinamento, execute o script `train_cdcgan.py`. Todas as configuraÃ§Ãµes (tamanho do lote, Ã©pocas, caminhos) estÃ£o no dicionÃ¡rio `CONFIG` no topo do arquivo.

  * O script criarÃ¡ a pasta `checkpoints_heavy` para salvar os modelos.
  * Checkpoints `.pth` completos sÃ£o salvos (incluindo estados do otimizador).
  * Modelos de gerador independentes sÃ£o salvos (ex: `G_epoch_500.pth` e `G_ema_epoch_500.pth`) para facilitar a geraÃ§Ã£o.
  * Uma grade de amostras fixas serÃ¡ salva em `generated_heavy` a cada `sample_every` Ã©pocas.

### 4. GeraÃ§Ã£o de Amostras

ApÃ³s o treinamento, vocÃª pode usar `generate_samples.py` para gerar um grande nÃºmero de imagens para cada classe.

1.  Edite o `CONFIG` em `generate_samples.py` para apontar para o checkpoint do gerador que vocÃª deseja usar (recomenda-se o `G_ema_epoch_X.pth`).

    ```python
    CONFIG = {
        'checkpoint': 'checkpoints_heavy/G_ema_epoch_500.pth', # <--- Mude aqui
        'out_dir': 'generated_heavy/epoch500_ema_inference', # <--- Mude o diretÃ³rio de saÃ­da
        'num_per_class': 100,
        # ...
    }
2.  Execute o script: `python generate_samples.py`

Isso criarÃ¡ o diretÃ³rio `out_dir` especificado e o preencherÃ¡ com subpastas para cada classe, cada uma contendo `num_per_class` imagens geradas.

-----